\chapter{Linux 内核开发}

\section{启动}

关于 Linux 的启动分析（ARMv8 架构），我们专门在另外一个文档撰写。
所以在此不再浪费篇幅。

\section{内存管理}

\section{task}

\section{网络}

\section{内核开发}

\section{驱动架构及子系统}
\subsection{UART}

\subsection{Timer}

\subsection{WDT}

\subsection{DMA}

\subsubsection{DMA 引擎控制器}

如果您要构建一个 DMA 控制器驱动，那么需要查阅这个文档
\url{https://www.kernel.org/doc/Documentation/dmaengine/provider.txt}。

\subsubsection{DMA 客户端和接口使用}

如果您需要在设备驱动中使用 DMA，那么以下文档是您必须要理解的。

\begin{itemize}
  \item \href{https://www.kernel.org/doc/Documentation/DMA-API.txt}{DMA 映射接口介绍}
  \item \href{https://www.kernel.org/doc/Documentation/DMA-API-HOWTO.txt}{DMA 动态映射接口指南}
  \item \href{https://www.kernel.org/doc/Documentation/dmaengine/client.txt}{DMA 引擎 API 指南}
\end{itemize}

如果对上述的文档介绍比较迷糊，那么可以参考内核驱动代码的实际例子。
比如，可以参考 pl011 串口驱动中对 DMA 的使用（drivers/tty/serial/amba-pl011.c）。

根据 DMA 映射接口相关文档的介绍，一般 DMA 映射有：

\begin{enumerate}
  \item 使用大型 DMA-coherent 缓冲区。
    \begin{lstcode}
      void *
      dma_alloc_coherent(struct device *dev, size_t size,
          dma_addr_t *dma_handle, gfp_t flag);
      void
      dma_free_coherent(struct device *dev, size_t size, void *cpu_addr,
          dma_addr_t dma_handle);
    \end{lstcode}
  \item 使用小型 DMA-coherent 缓冲区。
    \begin{lstcode}
      struct dma_pool *
      dma_pool_create(const char *name, struct device *dev,
          size_t size, size_t align, size_t alloc);
      void *
      dma_pool_zalloc(struct dma_pool *pool, gfp_t mem_flags,
          dma_addr_t *handle);
      void *
      dma_pool_alloc(struct dma_pool *pool, gfp_t gfp_flags,
          dma_addr_t *dma_handle);
      void
      dma_pool_free(struct dma_pool *pool, void *vaddr,
          dma_addr_t addr);
      void
      dma_pool_destroy(struct dma_pool *pool);
    \end{lstcode}
  \item DMA 地址限制
    \begin{lstcode}
      int
      dma_set_mask_and_coherent(struct device *dev, u64 mask);
      int
      dma_set_mask(struct device *dev, u64 mask);
      int
      dma_set_coherent_mask(struct device *dev, u64 mask);
      u64
      dma_get_required_mask(struct device *dev);
      size_t
      dma_max_mapping_size(struct device *dev);
      unsigned long
      dma_get_merge_boundary(struct device *dev);
    \end{lstcode}
  \item 流式 DMA 映射
    \begin{lstcode}
      dma_addr_t
      dma_map_single(struct device *dev, void *cpu_addr, size_t size,
          enum dma_data_direction direction)
    \end{lstcode}
    以下是 direction 枚举类型：
    \begin{stblr}
      {direction 枚举类型说明}
      {dir-enum}
      {ll}
      \hline[1pt]
      方向 & 说明 \\
      \hline
      DMA\_NONE            & no direction (used for debugging) \\
      DMA\_TO\_DEVICE      & data is going from the memory to the device \\
      DMA\_FROM\_DEVICE    & data is coming from the device to the memory \\
      DMA\_BIDIRECTIONAL  & direction isn't known \\
      \hline[1pt]
    \end{stblr}
\end{enumerate}

根据 DMA 引擎 API 指南中的接口介绍，使用 DMA 传输功能我们需要完成以下步骤：

\begin{enumerate}
  \item 申请一个 DMA slave channel。
    使用到的接口为：

    \lstinline!struct dma_chan *dma_request_chan(struct device *dev, const char *name);!

  \item 向控制器和 slave 设置特定参数。
    接口为：

    \lstinline!int dmaengine_slave_config(struct dma_chan *chan, struct dma_slave_config *config);!

  \item 获取一个事务描述符。
    由于有三种不同的传输模式，所以有以下三种接口：
    \begin{lstlisting}
      struct dma_async_tx_descriptor *dmaengine_prep_slave_sg(
        struct dma_chan *chan, struct scatterlist *sgl,
        unsigned int sg_len, enum dma_data_direction direction,
        unsigned long flags);

      struct dma_async_tx_descriptor *dmaengine_prep_dma_cyclic(
        struct dma_chan *chan, dma_addr_t buf_addr, size_t buf_len,
        size_t period_len, enum dma_data_direction direction);

      struct dma_async_tx_descriptor *dmaengine_prep_interleaved_dma(
        struct dma_chan *chan, struct dma_interleaved_template *xt,
        unsigned long flags);
    \end{lstlisting}
  \item 提交事务。
    描述符准备到位后将其提交给 DMA 引擎驱动的等待队列。

    \lstinline!dma_cookie_t dmaengine_submit(struct dma_async_tx_descriptor *desc);!

  \item 发起等待的 DMA 请求并且等待获取回调通知。
    接口为：

    \lstinline!void dma_async_issue_pending(struct dma_chan *chan);!
\end{enumerate}

以上只是简单的提炼，具体细节可以查阅相关文档。

\subsubsection{scatterlist}

关于 scatterlist，可以参阅 \href{https://lwn.net/Articles/234617/}{LWN 介绍文章}，下面是该文章的翻译。

\begin{quote}

高性能 I/O 通常涉及使用直接内存访问（DMA）操作。
通过 DMA，I/O 设备可以在不干扰 CPU 的情况下直接与主内存交换数据。
在最简单的 DMA 形式中，控制器被赋予一个指向内存区域的指针，给定长度，并指示其进行操作。
处理器随后可以忘记这个操作，直到设备发出工作完成的信号。

然而，这种简单的视图有一个缺点，即它假设要传输的数据在内存中是连续存储的。
当I/O 缓冲区在内核空间时，内核通常可以安排它在物理上是连续的——尽管随着缓冲区大小的增加，这变得越来越困难。
如果缓冲区在用户空间，则保证它分散在物理内存中。
因此，如果 DMA 操作能够处理被分割成多个不同部分的缓冲区，那将是非常理想的。

事实上，对于任何具有合理能力的外围设备，缓冲区都可以这样分割。
对这种缓冲区进行操作的术语是“散 / 聚集 I/O”；
Linux 早已很好地支持散 / 聚集。
Linux 设备驱动程序的 DMA 章节详细介绍了散 / 聚集。
简而言之，驱动程序首先填充一个散列表结构数组，这在 i386 架构上看起来像这样：

\begin{lstlisting}[language=C]
struct scatterlist {
    struct page *page;
    unsigned int    offset;
    dma_addr_t  dma_address;
    unsigned int    length;
};
\end{lstlisting}

对于每个段，page 指针指示该段在内存中的位置，offset 指示数据在页面内的起始位置，length 是段的长度。
列表填充完成后，驱动程序调用：

\begin{lstlisting}[language=C]
int dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,
               enum dma_data_direction direction);
\end{lstlisting}

此操作至少会填充每个散列表条目的 dma\_address 字段，该地址可以提供给外围设备。
然而，它可能做的不止这些：物理上连续的页面可能会合并为单个散列表条目，或者系统的 I/O 内存管理单元可能会被编程，使得从设备的角度看，列表的部分（或全部）在虚拟上是连续的。
所有这些——包括 struct scatterlist 的确切形式——都是架构相关的，但散 / 聚集接口的设置使得驱动程序无需担心架构细节。

最近，散 / 聚集接口中的一个特定限制显现出来。
由于各种原因，散列表必须适应单个页面；
这一限制对可以表示的段数设置了上限。
在启用高内存的 i386 架构上，struct scatterlist 需要 20 字节，这限制了散列表为 204 个条目。
如果每个散列表条目指向一个完整页面，DMA 传输的最大大小将约为 800KB。
在x86 - 64 系统上，情况更糟：结构体使用 40 字节，将最大长度减半。

在某些情况下，需要更大的 I/O 操作。
块I/O 子系统是其中之一，但肯定还有其他情况：高分辨率视频捕获设备就是一个例子。
散列表长度的限制是激励开发人员工作的因素之一，他们致力于支持更大的块大小。
通过增加有效页面大小，他们能够增加最大 I/O 操作大小。

然而，增加页面大小并不是唯一可行的方法；
另一种方法是简单地增加散列表的长度。
多页面连续的散列表实际上并不可行，但可以链式连接单页面散列表。
Jens Axboe 一直在研究这种方法；
截至本文撰写时，他的散列表链式连接补丁已经是第六版。

链式连接通过重载页面指针在页面中的最后一个散列表条目来完成。
设置最低有效位以指示该条目实际上是链条链接，而不是另一个要传输的段。
对驱动程序来说，这个改变几乎是透明的。
在当前内核中，遍历散列表的代码通常看起来像这样：

\begin{lstlisting}[language=C]
struct scatterlist *sg = &the_scatterlist[0];

for (i = 0; i < nentries; i++) {
    program_io_operation(sg);
    sg++;
}
\end{lstlisting}

当使用链式连接时，简单地通过数组递增将不再有效。
所以 Jens 添加了一个简单的 sg\_next() 宏，当需要时跟随链条链接。
因此，上述 sg++ 行变成如下所示：

\begin{lstlisting}[language=C]
sg = sg_next(sg);
\end{lstlisting}

由于需要对驱动程序进行更改，除非确定驱动程序已准备好，否则不应使用链式散列表。
Jens 的补丁修复了许多驱动程序，特别是在块子系统中。
即便如此，最大 I/O 大小必须由管理员通过 sysfs 文件显式提高，然后链式连接才会启用。
一旦启用，多兆字节的 I/O 操作就变得可能。
无需进行侵入性的内存管理更改。
\end{quote}

\subsection{I2C/I3C}

\subsection{SPI/QSPI}

\subsection{CAN}

\subsection{Ethernet}

\subsection{USB}

\subsection{PCI/PCIe}

\subsubsection{设备驱动}

添加 PCIe 设备驱动需要的头文件。

\begin{lstlisting}[language=C]
#include <linux/module.h>
#include <linux/pci.h>
#include <linux/slab.h>
#include <linux/miscdevice.h>
\end{lstlisting}

顺便我们先将 PCIe 设备的 ID（PID 与 VID）填充到 \lstinline!pci_device_id! 数组中。

\begin{lstlisting}[language=C]
static struct pci_device_id my_pcidev_ids[] = {
	{ PCI_DEVICE(0x1234, 0x5678) },
	{ },
};
MODULE_DEVICE_TABLE(pci, my_pcidev_ids);
\end{lstlisting}

下面我们构建模块初始化函数，一般从文件的底部开始构建。

\begin{lstlisting}[language=c]
  module_init(my_pcidev_init);
  module_exit(my_pcidev_exit);

  MODULE_AUTHOR("Jason Wang (wang_borong@163.com)");
  MODULE_LICENSE("GPL v2");
  MODULE_DESCRIPTION("A simple kernel pci device driver");
  MODULE_VERSION("0.1");
\end{lstlisting}

让我们继续向上实现 \lstinline!my_pcidev_init! 和 \lstinline!my_pcidev_exit!。

\begin{lstlisting}
static struct pci_driver my_pcidev_drv = {
	.name = "my_pcidev",
	.id_table = my_pcidev_ids,
	.probe = my_pcidev_probe,
	.remove = my_pcidev_remove,
};

static int __init my_pcidev_init(void)
{
	return pci_register_driver(&my_pcidev_drv);
}

static void __exit my_pcidev_exit(void)
{
	pci_unregister_driver(&my_pcidev_drv);
}
\end{lstlisting}

可以看到 \lstinline!pci_register_driver! 注册了 PCIe 设备的驱动结构体实例。
而我们就在这个结构体中填充 probe 和 remove 回调函数。
设备的驱动初始化就在它的 probe 函数中实现。

\begin{lstlisting}[language=C]

struct test_data {
	struct pci_dev *pdev;
	void __iomem *ptr_bar0;
};

static int my_pcidev_probe(struct pci_dev *dev, const struct pci_device_id *pci_id)
{
	int status;
	int id;
	char name[24];
	struct test_data *my_data;
	struct miscdevice *misc_device;

	status = pci_resource_len(dev, 0);
	pr_info("my_pcidev: bar0 is %d bytes in size\n", status);
	status = pci_resource_len(dev, 1);
	pr_info("my_pcidev: bar1 is %d bytes in size\n", status);

	status = pcim_enable_device(dev);
	if (status < 0) {
		pr_info("my_pcidev: Could not enable device\n");
		return status;
	}

	status = pcim_iomap_regions(dev, BIT(0), DRV_MODULE_NAME);
	if (status < 0) {
		pr_info("my_pcidev: BAR0 is already in use! (ret: %d)\n", status);
		goto err_disable_pdev;
	}

	status = pcim_iomap_regions(dev, BIT(1), DRV_MODULE_NAME);
	if (status < 0) {
		pr_info("my_pcidev: BAR1 is already in use! (ret: %d)\n", status);
		goto err_disable_pdev;
	}

	my_data = devm_kzalloc(&dev->dev, sizeof(struct test_data), GFP_KERNEL);
	if (my_data == NULL) {
		status = -ENOMEM;
		pr_info("my_pcidev: Error! Out of memory\n");
		goto err_iounmap;
	}

	my_data->ptr_bar0 = pcim_iomap_table(dev)[0];
	if (my_data->ptr_bar0 == NULL || my_data->ptr_bar1 == NULL) {
		status = -ENOMEM;
		pr_info("my_pcidev: BAR0 pointer is invalid\n");
		goto err_iounmap;
	}

	pci_set_master(dev);

	my_data->pdev = dev;
	pci_set_drvdata(dev, my_data);
  // ...
  // 我们省略了后面的异常处理
}

static void my_pcidev_remove(struct pci_dev *dev)
{
	int id;
	struct test_data *my_data = pci_get_drvdata(dev);

	pcim_iounmap_regions(dev, BIT(0));
	pcim_iounmap_regions(dev, BIT(1));
	pci_disable_device(dev);
	devm_kfree(&dev->dev, my_data);
}
\end{lstlisting}

以上是一个基本的 PCIe 设备驱动框架，如果需要其他的功能需要相应的添加。

\subsubsection{PCIe 解析}

\paragraph{配置空间}

每个 PCI 设备都有自己的配置空间，这个配置空间存在于 PCI 设备的内部空间中。
PCIe 主设备枚举到设备后，会将设备的配置空间映射到一块分配的内存地址中。
主设备通过 lspci 可以看到系统枚举到的所有 PCI 设备以及他们对应的配置空间。

\begin{lstlisting}
01:00.0 Non-Volatile memory controller: Biwin Storage Technology Co., Ltd. HP EX950 NVMe SSD (rev 03)
02:00.0 Ethernet controller: Realtek Semiconductor Co., Ltd. RTL8111/8168/8211/8411 PCI Express Gigabit Ethernet Controller (rev 02)
\end{lstlisting}

从以上的输出可以看到前面的数字标号（16 进制）如 \lstinline!02:00.0!。
这些标号的意义为，总线编号（Bus Number）、设备编号（Device Number）和功能编号（Function Number）。
协议规定系统中可以存在 256 个总线，每个总线可以挂载 32 个设备，每个设备可以有 8 个功能。
如果使用 lspci -D 去查看设备，你会发现最前面还有一列数字，这列数字是 Domain Number。

使用 lspci 可以指定查看某个设备的配置空间。

\begin{lstlisting}
> lspci -s 02:00.0 -x
02:00.0 Ethernet controller: Realtek Semiconductor Co., Ltd. RTL8111/8168/8211/8411 PCI Express Gigabit Ethernet Controller (rev 02)
00: ec 10 68 81 07 00 10 00 02 00 00 02 10 00 00 00
10: 01 40 00 00 00 00 00 00 04 00 10 84 00 00 00 00
20: 0c 00 00 00 60 00 00 00 00 00 00 00 ec 10 23 01
30: 00 00 00 00 40 00 00 00 00 00 00 00 ff 01 00 00
\end{lstlisting}

实际上，我们也可以从 sysfs 中获取配置空间内容，这也是 lspci 的获取途径。

\begin{lstlisting}
> hexyl --no-characters --border=none /sys/bus/pci/devices/0000:02:00.0/config
 00000000  ec 10 68 81 07 00 10 00   02 00 00 02 10 00 00 00  
 00000010  01 40 00 00 00 00 00 00   04 00 10 84 00 00 00 00  
 00000020  0c 00 00 00 60 00 00 00   00 00 00 00 ec 10 23 01  
 00000030  00 00 00 00 40 00 00 00   00 00 00 00 ff 01 00 00
\end{lstlisting}

协议规定配置空间有两种类型，一种用于普通的 EP 设备，一种用于 Bridge 设备。



\section{文件系统}

\subsection{ext2 简介}
